---
title: "Answers of StatComp Homework"
author: "RayLiu"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Answers of StatComp Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
# Overview

__StatComp20062__ also provides the homework answers for the 'Statistical Computing' course. 

#Homework 1
## Question

2.Use knitr to produce 3 examples in the book. The 1st example
should contain texts and at least one figure. The 2nd example
should contains texts and at least one table. The 3rd example
should contain at least a couple of LaTeX formulas.

## Answer
1)The plot below is generated from the formula:$\frac{1}{(1 + e^{-x+10})}$
```{r,echo=FALSE}
library(ggplot2)
myfun <- function(xvar) {
1/(1 + exp(-xvar + 10))
}
ggplot(data.frame(x=c(0, 20)), aes(x=x)) + stat_function(fun=myfun, geom="line")
```

***

2)The table below represents the cor matrix of the dataset "*rock*"

```{r,echo=FALSE}
data(rock)
x_html <- knitr::kable(cor(rock), "html")
kableExtra::kable_styling(x_html,bootstrap_options = "striped",
                          full_width = F,font_size = 14)
```
***
3)$\mathbb{P}$ is the probability measures on$(\mathbb{R},\mathscr{B})$,$F(x)=\mathbb{P}((-\infty,x])$,We will give the definition of the distribution function:

    * $F$ is not descending 
    
  + $\lim_{x \to -\infty}F(x)=0,\lim_{x \to \infty}F(x)=1$
    
  + $F$ is right continous,i.e.,$\lim_{y \downarrow x}F(y)=F(x)$

#Homework 2
## Question 3.3
The Pareto(a, b) distribution has cdf
$$F(x)=1-\left(\frac{b}{x} \right)^a,\qquad x\geqslant b>0,a>0$$
Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse
transform method to simulate a random sample from the Pareto(2, 2) distribution.
Graph the density histogram of the sample with the Pareto(2, 2)
density superimposed for comparison.

## Answer
solve the inverse function of Pareto distribution function 
$$F^{-1}\left(u\right)=\frac{b}{(1-u)^{\frac{1}{a}}}$$
take in the parameters$a=2,b=2$
$$F^{-1}\left(u\right)=\frac{2}{(1-y)^{\frac{1}{2}}}$$
the density of Pareto distribution
$$f(x)=\frac{ab^a}{x^{a+1}} \qquad x\geqslant b>0,a>0$$
$$f(x)=\frac{8}{x^3}$$
generate the random number using inverse transformation method
```{r}
a <- set.seed(3.3)
n <- 1000
u <- runif(n)
x <- 2/sqrt(1-u)
hist(x, prob = TRUE,breaks = seq(range(x)[1],range(x)[2],length.out = 31),main = bquote(2/sqrt(1-y)),xlim = c(0,60)) #density histogram of sample
y <- seq(0, 50, .01)
lines(y, 8/y^3) #density curve f(x)
```

***

## Question 3.9
The rescaled Epanechnikov kernel [85] is a symmetric density function
 \begin{equation}
    f_e\left(x\right)=\frac{3}{4}\left(1-x^2\right),\qquad\left| x\right|\leqslant 1  
    \end{equation}
$Devroye\quad and\quad Gy\ddot orfi$ [71, p. 236] give the following algorithm for simulation
from this distribution. Generate iid $U_1, U_2, U_3 ∼ U(−1, 1)$. If $|U_3| \geqslant |U_2|$ and $|U_3| \geqslant |U_1|$, deliver $U_2$; otherwise deliver $U_3$. Write a function
to generate random variates from $f_e$, and construct the histogram density
estimate of a large simulated random sample.

## Answer
according to the idea of $Devroye\quad and\quad Gy\ddot orfi$, we can write the codes like below；
```{r}
Epan <- function(n){
  a <- runif(n,-1,1)
  b <- runif(n,-1,1)
  c <- runif(n,-1,1)
  d <- ifelse(abs(c)>=abs(a) & abs(c)>=abs(b),b,c)
  return(d)
}
set.seed(3.9)
x <- Epan(1000)
hist(x, prob = TRUE,breaks = seq(range(x)[1],range(x)[2],length.out = 11)) #density histogram of sample
y <- seq(-1,1,.01)
lines(y, 3*(1-y^2)/4) #density curve f(x)
```

***

## Question 3.10
Prove that the algorithm given in Exercise 3.9 generates variates from the
density $f_e$ .

## Answer
proof：

\begin{align}           
&\because U_1,U_2,U_3\sim U(-1,1)&\\
&\therefore|U_1|,|U_2|,|U_3|\sim U(0,1)&
\end{align}
details:(take $U_1$ for example)
\begin{align} 
&f(u_1)=\frac{1}{2},-1\leqslant u_1\leqslant 1\\
&F(u_1)=\frac{1}{2}(x+1),-1\leqslant u_1\leqslant 1
\end{align}
\begin{align}
F_{U_1}(x)
&=P(|U_1|\leqslant x)&\\
&=P(-x\leqslant U_1\leqslant x)&\\
&=F_{U_1}(x)-F_{U_1}(-x)&\\
&=\frac{1}{2}(x+1)-\frac{1}{2}(-x+1)&\\
&=x&\\
&\therefore |U_i| \sim U(0,1),i=1,2,3
\end{align}
\begin{align}
&\because U_1,U_2,U_3 independent&\\
&\therefore we can get the joint density function of these three r.v. f_{U_1U_2U_3}(u_1,u_2,u_3)=\frac{1}{8},-1\leqslant u_1\leqslant 1,-1\leqslant u_2\leqslant 1,-1\leqslant u_3\leqslant 1&\\
P(X\leqslant x)&=P(X\leqslant x,X=U_2)+P(X\leqslant x,X=U_3)\\
&=P(U_2\leqslant x,|U_3|\geqslant |U_2|,|U_3|\geqslant |U_1|)+P(U_3\leqslant x,|U_3|\leqslant |U_2|,|U_3|\geqslant |U_1|)+P(U_3\leqslant x,|U_3|\geqslant |U_2|,|U_3|\leqslant |U_1|)+P(U_3\leqslant x,|U_3|\leqslant |U_2|,|U_3|\leqslant |U_1|)\\
\end{align}
computing the probabilty of four situations respectively: 

* 1.$P(U_2\leqslant x,|U_3|\geqslant |U_2|,|U_3|\geqslant |U_1|)$\\
the prob of this space$\{|U_3|\geqslant |U_2|,|U_3|\geqslant |U_1|\}$ is:

\begin{align}
P(|U_3|\geqslant |U_2|,|U_3|\geqslant |U_1|)&=\iiint_{\mathcal D} \, d|u_1|\,d|u_2|\,d|u_3|\\
&=\int_0^1\int_0^{|u_3|}\int_0^{|u_3|}\, d|u_1|\,d|u_2|\,d|u_3|\\
&=\int_0^1\int_0^{z}\int_0^{z}\, dx\,dy\,dz\\
&=\int_0^1 z^2\, dz\\
&=\frac{1}{3}z^3|_0^1\\
&=\frac{1}{3}
\end{align}

then, we computing the integral after removing the absolute value

$$\left\{
\begin{aligned}
U_1\leqslant |U_3|&,and\\
U_1\geqslant -|U_3|
\end{aligned} \right.\quad \left\{
\begin{aligned}
U_3\geqslant |U_2|&,or \\
U_3\leqslant -|U_2|
\end{aligned}\right.$$

discussing $U_3\geqslant |U_2|$
\begin{align}
P(U_2\leqslant x,U_3\geqslant |U_2|,|U_3|\geqslant |U_1|)&=\iiint_{\mathcal D}\frac{1}{8} \,du_1\,du_2\,du_3\\
&=\int_{-1}^x\int_{|u_2|}^1\int_{-|u_3|}^{|u_3|}\frac{1}{8} \,du_1\,du_2\,du_3\\
&=\int_{-1}^x\int_{|u_2|}^1\frac{1}{4}|u_3| \,du_1\,du_2\,du_3\\
&=\int_{-1}^x\int_{|u_2|}^1\frac{1}{4}u_3 \,du_1\,du_2\,du_3\\
&=\int_{-1}^x\frac{1}{8}u_3^2|_{|u_3|}^1 \,du_3\\
&=\frac{1}{8}[x-\frac{1}{3}x^3+\frac{2}{3}]
\end{align}

discussing $U_3\leqslant -|U_2|$,the result is as same,so
\begin{align}
P(U_2\leqslant x,|U_3|\geqslant |U_2|,|U_3|\geqslant |U_1|)&=\frac{1}{8}[x-\frac{1}{3}x^3+\frac{2}{3}]\times 2\\
&=\frac{1}{4}[x-\frac{1}{3}x^3+\frac{2}{3}]
\end{align}

+ 2.$P(U_3\leqslant x,|U_3|< |U_2|,|U_3|< |U_1|)$\\
the prob of this space$\{|U_3|< |U_2|,|U_3|< |U_1|\}$ is:(令$X=|U_1|,Y=|U_2|,Z=|U_3|$)

\begin{align}
P(|U_3|< |U_2|,|U_3|< |U_1|)&=\iiint_{\mathcal D} \, d|u_1|\,d|u_2|\,d|u_3|\\
&=\int_0^1\int_z^{1}\int_z^{1}\, dx\,dy\,dz\\
&=\int_0^1 (1-z)^2\, dz\\
&=z-z^2+\frac{1}{3}z^3|_0^1\\
&=\frac{1}{3}
\end{align}

then, we computing the integral after removing the absolute value

$$\left\{
\begin{aligned}
U_1> |U_3|&,or\\
U_1< -|U_3|
\end{aligned} \right.\quad \left\{
\begin{aligned}
U_2> |U_3|&,or\\
U_2< -|U_3|
\end{aligned}\right.$$

discussing$U_2> |U_3|,U_1> |U_3|$
\begin{align}
P(U_3\leqslant x,|U_3|< U_2,|U_3|< U_1)&=\iiint_{\mathcal D}\frac{1}{8} \,du_1\,du_2\,du_3\\
&=\int_{-1}^x\int_{|u_3|}^1\int_{|u_3|}^1\frac{1}{8} \,du_1\,du_2\,du_3\\
&=\int_{-1}^x\frac{1}{8}(1-|u_3|)^2 \,du_3\\
&=\left\{\begin{aligned}
&\frac{1}{24}(1+x)^3&,x<0\\
&\frac{1}{12}-\frac{1}{24}(1-x)^3&,x\geqslant 0
\end{aligned} \right.\\
\end{align}

discussing the other three situations, finding the same results, so
\begin{align}
P(U_3\leqslant x,|U_3|< |U_2|,|U_3|< |U_1|)&=4\times\left\{\begin{aligned}
&\frac{1}{24}(1+x)^3&,x<0\\
&\frac{1}{12}-\frac{1}{24}(1-x)^3&,x\geqslant 0
\end{aligned} \right.\\
&=\left\{\begin{aligned}
&\frac{1}{6}(1+x)^3&,x<0\\
&\frac{1}{3}-\frac{1}{6}(1-x)^3&,x\geqslant 0
\end{aligned} \right.
\end{align}

+ 3.$P(U_3\leqslant x,|U_3|< |U_2|,|U_3|\geqslant |U_1|)$\\
the prob of this space$\{ |U_3|< |U_2|,|U_3|\geqslant |U_1|\}$ is:

\begin{align}
P(|U_3|\geqslant |U_2|,|U_3|\geqslant |U_1|)&=(1-\frac{1}{3}-\frac{1}{3})/2
&=\frac{1}{6}
\end{align}

then, we computing the integral after removing the absolute value

$$\left\{
\begin{aligned}
U_1\leqslant |U_3|&,and\\
U_1\geqslant -|U_3|
\end{aligned} \right.\quad \left\{
\begin{aligned}
U_2> |U_3|&,or\\
U_2< -|U_3|
\end{aligned}\right.$$

discussing $U_2> |U_3|$
\begin{align}
P(U_3\leqslant x,|U_3|< U_2,|U_3|\geqslant |U_1|)&=\iiint_{\mathcal D}\frac{1}{8} \,du_1\,du_2\,du_3\\
&=\int_{-1}^x\int_{|u_3|}^1\int_{-|u_3|}^{|u_3|}\frac{1}{8} \,du_1\,du_2\,du_3\\
&=\int_{-1}^x\frac{1}{4}(1-|u_3|)|u_3| \,du_3\\
&=\left\{\begin{aligned}
&\frac{1}{24}-\frac{1}{8}x^2-\frac{1}{12}x^3&,x<0\\
&\frac{1}{24}+\frac{1}{8}x^2-\frac{1}{12}x^3&,x\geqslant 0
\end{aligned} \right.\\
\end{align}

we find the computing result of last situation is as same as before, so:
\begin{align}
P(U_3\leqslant x,|U_3|< |U_2|,|U_3|\geqslant |U_1|)&=2\times\left\{\begin{aligned}
&\frac{1}{24}-\frac{1}{8}x^2-\frac{1}{12}x^3&,x<0\\
&\frac{1}{24}+\frac{1}{8}x^2-\frac{1}{12}x^3&,x\geqslant 0
\end{aligned} \right.\\
&=\left\{\begin{aligned}
&\frac{1}{12}-\frac{1}{4}x^2-\frac{1}{6}x^3&,x<0\\
&\frac{1}{12}+\frac{1}{4}x^2-\frac{1}{6}x^3&,x\geqslant 0
\end{aligned} \right.
\end{align}

+ 4.$P(U_3\leqslant x,|U_3|\geqslant |U_2|,|U_3|< |U_1|)$
This situation is equal to 3.

At all：
\begin{align}
P(X\leqslant x)&=P(X\leqslant x,X=U_2)+P(X\leqslant x,X=U_3)\\
&=P(U_2\leqslant x,|U_3|\geqslant |U_2|,|U_3|\geqslant |U_1|)+P(U_3\leqslant x,|U_3|\leqslant |U_2|,|U_3|\geqslant |U_1|)+P(U_3\leqslant x,|U_3|\geqslant |U_2|,|U_3|\leqslant |U_1|)+P(U_3\leqslant x,|U_3|\leqslant |U_2|,|U_3|\leqslant |U_1|)\\
&=\frac{1}{4}[x-\frac{1}{3}x^3+\frac{2}{3}]+2\times\left\{\begin{aligned}
&\frac{1}{12}-\frac{1}{4}x^2-\frac{1}{6}x^3&,x<0\\
&\frac{1}{12}+\frac{1}{4}x^2-\frac{1}{6}x^3&,x\geqslant 0
\end{aligned} \right.
+\left\{\begin{aligned}
&\frac{1}{6}(1+x)^3&,x<0\\
&\frac{1}{3}-\frac{1}{6}(1-x)^3&,x\geqslant 0
\end{aligned} \right.\\
&=\frac{3}{4}x-\frac{1}{4}x^3+\frac{1}{2},-1<x<1
\end{align}

solve the density function by distribution function
\begin{equation}
    f_e\left(x\right)=\frac{3}{4}\left(1-x^2\right),\qquad\left| x\right|\leqslant 1  
    \end{equation}
    
***

## Question 3.13
It can be shown that the mixture in Exercise 3.12 has a Pareto distribution
with cdf
$$F(y)= 1 − \left(\frac{\beta}{\beta+y}\right)^r,\qquad y\geqslant0. $$
(This is an alternative parameterization of the Pareto cdf given in Exercise
3.3.) Generate 1000 random observations from the mixture with $r = 4$ and
$β = 2$. Compare the empirical and theoretical (Pareto) distributions by graphing
the density histogram of the sample and superimposing the Pareto density
curve.

## Answer
let us make use of the method3.12 to simulate the distribution 
where $f(y)=r\beta^r(\beta+y)^{-r-1}$
```{r}
a <- set.seed(3.13)
n <- 1000
ga <- rgamma(1000,4,2)
y <- rexp(1000,ga)
hist(y, prob = TRUE,breaks = seq(range(y)[1],range(y)[2],length.out = 31),main = "PARETO=GAMMA+exp(gamma)") #density histogram of sample
x <- seq(0, 10, .01)
lines(x, 4*2^4*(2+x)^-5) #density curve f(x)
```

#Homework 3
## Question 5.1
Compute a Monte Carlo estimate of
$$\int_0^{\pi/3}\sin tdt$$
and compare your estimate with the exact value of the integral.

## Answer
1. Assign the number m.
2. Generate X1, . . .,Xm, iid from Uniform(0, $\pi/3$).
3. Compute $\hat\theta=\frac{1}{m}\sum_{i=1}^m(\frac{3}{\pi}\sin X_i)$.
4. Return $\hat\theta$ and accurate value.

```{r}
a <- set.seed(5.1)
m <- 1e4; x <- runif(m, min=0, max=pi/3)
theta.hat <- mean(sin(x)) * pi/3
print(c(theta.hat,-(cos(pi/3)-cos(0))))
```

***

## Question 5.7
Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6

## Answer

```{r}
a <- set.seed(5.7)
m <- 1e5; x <- runif(m)
T1 <- exp(x)
theta1.hat <- mean(T1)#simple MC
x2 <- x[1:m/2]
T2 <- (exp(x2)+exp(1-x2))/2
theta2.hat <- mean(T2)#antithetic variate approach
show <- c(theta1.hat,theta2.hat,exp(1)-1)
names(show) <- c("SMC","antithetic","theoretical integral value")
print(show)
eprv <- (var(T1)-var(T2))/var(T1)
var.g <- (exp(1)^2-1)/2-(exp(1)-1)^2
var.theta <- var.g/m
var.g1 <- ((exp(1)^2-1)/2-(exp(1)-1)^2)*2+2*(exp(1)-(exp(1)-1)^2)
var.theta1 <- var.g1/(2*m)
tprv <- (var.theta- var.theta1)/var.theta
show2 <- c(eprv,tprv)
names(show2) <- c("empirical prv","theoretical prv")
print(show2)

```
As the result show above, we know that:

1. The integral computing through antithetic method is closer to the true value the SMC.
2. Although the empirical estimate of the percentage reduction in variance is really close to the theoretical one given in the (5.6), it's biased which needs to be concerned.

***

## Question 5.11
If $\hat\theta_1$ and $\hat\theta_2$ are unbiased estimators of $\theta$, and $\hat\theta_1$ and $\hat\theta_2$ are antithetic, we derived that c∗ = 1/2 is the optimal constant that minimizes the variance of $\hat\theta_c = c\hatθ_1 + (1 − c)\hatθ_2$. Derive c∗ for the general case. That is, if $\hat\theta_1$ and $\hat\theta_2$ are any two unbiased estimators of θ, find the value $c^*$  that minimizes the variance of the estimator $\hat\theta_c = c\hatθ_1 + (1 − c)\hatθ_2$ in equation (5.11). ($c^*$ will be a function of the variances and the covariance of the estimators.)

## Answer
We try to rewrite the formula(5.11) and sort out the monomial that contains $Var(\hat\theta_2)$ , $Var(\hat\theta_1)$and$Cov(\hat\theta_2,\hat\theta_1)$ respectively.

$$\begin{align}
&Var(\hat\theta_2)+c^2Var(\hat\theta_1-\hat\theta_2)+2cCov(\hat\theta_2,\hat\theta_1-\hat\theta_2)\\
=&Var(\hat\theta_2)(c-1)^2+Var(\hat\theta_1)c^2+Cov(\hat\theta_2,\hat\theta_1)2c(1-c)\\
=&f(c)
\end{align}$$

We write the derivative function of $f(c)$ and solve it to find out the stationary point.

$$\begin{align}
f^{'}(c)&=2a(c-1)+2bc+2d(1-2c)\\
&=(2a+2b-4d)c+2d-2a
\end{align}$$
In the formula, a,b and d represent $Var(\hat\theta_2)$ , $Var(\hat\theta_1)$and$Cov(\hat\theta_2,\hat\theta_1)$respectively.

Pay attention to the coefficient $a+b-2d$, it's always more than 0 unless $\hat\theta_1\equiv\hat\theta_2$, because it means $Var(\hat\theta_2)+Var(\hat\theta_1)-2Cov(\hat\theta_2,\hat\theta_1)=Var(\hat\theta_1-\hat\theta_2)\geqslant 0$. Variance is always no less than zero.

Thus, we can find the stationary point is $c^*=\frac{a-d}{a+b-2d}=\frac{Var(\hat\theta_1)-Cov(\hat\theta_2,\hat\theta_1)}{Var(\hat\theta_1-\hat\theta_2)}$, which makes the$\hat\theta^*_c$ best.

#Homework 4
## Question 5.13
Find two importance functions $f_1$ and $f_2$ that are supported on $(0,+\infty)$ and are ‘close’ to
$$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2},\quad x>1$$
Which of your two importance functions should produce the smaller variance in estimating
$$\int_1^{+\infty}\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx$$
by importance sampling? Explain

## Answer
We can find 2 importance functions exponetial distribution
$$f_1=\lambda e^{-\lambda x}$$ and gamma distribution
$$f_2=\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$$
which are supported on $(0,+\infty)$
To close $g(x)$, we set the parameters $\lambda=1,\alpha=4,\beta=2$ and render the plot below

```{r}
x <- seq(0, 5, .01)
w <- 2
beta1 <- 2;alpha <- 4
f1 <- dexp(x)
f2 <- dgamma(x,4,2)
g <- x^2/sqrt(2*pi)*exp(-x^2/2)
#figure (a)
plot(x, g, type = "l", main = "", ylab = "",xlim = c(1,5),
ylim = c(0,2), lwd = w)
lines(x, f1, lty = 2, lwd = w)
lines(x, f2, lty = 3, lwd = w)
legend("topright", legend = c("g", 1:2),
lty = 1:3, lwd = w, inset = 0.02)
#figure (b)
plot(x, g/f1, type = "l", main = "", ylab = "",xlim = c(1,5),
ylim = c(0,20), lwd = w, lty = 1)
lines(x, g/f1, lty = 2, lwd = w)
lines(x, g/f2, lty = 3, lwd = w)
legend("topright", legend = c(1:2),
lty = 2:3, lwd = w, inset = 0.02)
```
It seems that Gamma(4,2) is more like $g(x)$ than exp(1).
```{r}
m <- 100000
theta.hat <- var.hat <- numeric(2)
gfun <- function(x) {
x^2/sqrt(2*pi)*exp(-x^2/2) * (x > 1)
}

x <- rexp(m) #using f1
fg <- gfun(x) / dexp(x)
theta.hat[1] <- mean(fg)
var.hat[1] <- var(fg)

x <- rgamma(m,4,2) #using f2
fg <- gfun(x) / dgamma(x,4,2)
theta.hat[2] <- mean(fg)
var.hat[2] <- var(fg)
rbind(theta.hat, var.hat)
```
The simulation result shows that $f_2$ has smaller variance of 0.064 compared to that of $f_1$ of 0.341, which supports the idea that $f_2$ is more close to $g$.

***

## Question 5.15
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10

## Answer
This method is called "Stratified Importance Sampling", which firstly uses important function to make sure that $g/f$ is close to a constant, and then avails of the less variance in each straty.
```{r}
M <- 10000 #number of replicates
k <- 5 #number of strata
r <- M / k #replicates per stratum
N <- 50 #number of times to repeat the estimation
T2 <- numeric(k)
estimates <- matrix(0, N, 2)
g <- function(x) {
exp(-x - log(1+x^2)) * (x > 0) * (x < 1)
}

q <- - log(1 - seq(0,1,0.2) * (1 - exp(-1)))#find the quantile

for (i in 1:N) {
  estimates[i, 1] <- mean(g(runif(M)))
  for (j in 1:k) {
    u <- runif(M/k)   
    x <- - log(exp(-q[j]) - u * (1 - exp(-1)) / 5)
    T2[j] <- mean(g(x)/(5*(exp(-x) / (1 - exp(-1)))))
  }
  estimates[i, 2] <- sum(T2)
}
mean.est <- apply(estimates, 2, mean)
l <- var.est <- apply(estimates, 2, var)
x_html <- knitr::kable(rbind(mean.est,var.est),"html")
kableExtra::kable_styling(x_html,bootstrap_options = "striped",
                          full_width = F,font_size = 14)
```
It seems like that the variance has been reducted one more step using Stratified Importance Sampling compared with just Stratified Method.

```{r}
vrp <- as.data.frame((l[1]-l[2])/l[1])#variance reduction
colnames(vrp) <- "vrp"
x_html <- knitr::kable(vrp,"html")
kableExtra::kable_styling(x_html,bootstrap_options = "striped",
                          full_width = F,font_size = 14)
```
We can find the percent reduction in variance is close to 100% compared to Stratified Method.
```{r,echo=F}
example510 <- matrix(c(0.5241140, 0.5313584, 0.5461507, 0.52506988 ,0.5260492,
0.2436559, 0.4181264, 0.9661300, 0.09658794, 0.1427685),2,5,byrow = T)
example510 <- cbind(example510,round(rbind(mean.est,sqrt(var.est)),digits = 7))
rownames(example510) <- c("theta.hat","se")
colnames(example510) <- c(NULL,"f1","f2","f3","f4","f5","stratified","importance stratum")
x_html <- knitr::kable(example510,"html")
kableExtra::kable_styling(x_html,bootstrap_options = "striped",
                          full_width = F,font_size = 14)
```
In sum, the Stratified Importance Sampling has the lowest standard deviation.

***

## Question 6.4
Suppose that $X_1,\dots,X_n$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter $\mu$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.

## Answer
According to the properties of lognormal distribution, we have$logX_i\sim N(\mu,\sigma^2)$, then $\frac{1}{n}\sum_{i=1}^nlogX_i\sim N(\mu,\sigma^2/n)$ using Central Limit Theorem. And $\frac{1}{n}\sum_{i=1}^nlogX_i$ is MLE of $\mu$ and unbiased. So we can create the confidence interval for the parameter $\mu$:
$$(\frac{1}{n}\sum_{i=1}^nlogX_i\pm\frac{\sigma}{\sqrt{n}}z_{\alpha/2})$$
Because of the unknown parameter of $\sigma$ , we use its plug-in estimator. At the moment, the distribution of pivot estimator is t-distribution.
$$(\frac{1}{n}\sum_{i=1}^nlogX_i\pm\frac{\hat\sigma}{\sqrt{n}}t(n-1)_{\alpha/2})$$
```{r}
set.seed(6.4)
n <- 20
alpha <- .05
calcCI <- function(n, alpha) {
x <- exp(rnorm(n,0,2))
return(c(mean(log(x)) - sd(log(x)) * qt(1-alpha/2, df = n-1) / sqrt(n),mean(log(x)) + sd(log(x)) * qt(1-alpha/2, df = n-1) / sqrt(n)))
}
CL <- replicate(1000, expr = calcCI(n,alpha) )
mean(0>CL[1,] & 0<CL[2,])
```
This CI is really good.

***

## Question 6.5
Suppose a 95% symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use aMonte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^2(2)$ data with sample size n = 20. Compare your t-interval results with the simulation results in Example 6.4. (The t-interval should be more robust to departures from normality than the interval for variance.)

## Answer
the confidence interval for a mean is:
$$(\frac{1}{n}\sum_{i=1}^nX_i\pm\frac{s}{\sqrt{n}}t(n-1)_{\alpha/2})$$

```{r}
set.seed(6.5)
n <- 20
alpha <- .05
calcCI <- function(n, alpha) {
x <- rchisq(n,2)
return(c(mean(x) - sd(x) * qt(1-alpha/2, df = n-1) / sqrt(n),mean(x) + sd(x) * qt(1-alpha/2, df = n-1) / sqrt(n)))
}
CL <- replicate(1000, expr = calcCI(n,alpha) )
mean(2>CL[1,] & 2<CL[2,])
```
This CI is liberal, but compared to the results in Example 6.4 (empirical coverage probability=0.95), the result is not so bad which shows robust.

#Homework 5
## Question 6.7
Estimate the power of the skewness test of normality against symmetric $Beta(\alpha,\alpha)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(\upsilon)$?

## Answer
First, let we look into how the symmetric $Beta(\alpha,\alpha)$ distributions look like.
```{r}
x <- seq(0,1,0.005)
plot(x,dbeta(x,1,1),type = "l",xlim = c(0.1,0.9),ylim = c(0,3.8),ylab = "density")
for (i in 1:10) {
  lines(x,dbeta(x,i,i),type = "l",col=i,lwd=2)
}
legend(0.7,4, legend = paste('a=',1:10), lty = c(1,1), col = 1:10)
```

Then we'll find that the power is less than 0.05 when $\alpha$ is among $[1,30]$, which shows that one can not tell a small sample is from whether a normal distribution or a symmetric beta distribution using the skewness test of normality.

```{r,warning=FALSE}
sk <- function(x) {
  #computes the sample skewness coeff.
  xbar <- mean(x)
  m3 <- mean((x - xbar)^3)
  m2 <- mean((x - xbar)^2)
  return( m3 / m2^1.5 )
}#compute skewness of sample

set.seed(6.71)
c.alpha <- .05
n <- 30
m <- 2000
pa.alpha <- seq(1,30,1)

N <- length(pa.alpha)
pwr <- numeric(N)
#critical value for the skewness test
cv <- qnorm(1-c.alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
for (j in 1:N) { 
  sktests <- numeric(m)
  for (i in 1:m) {
    x <- rbeta(n, pa.alpha[j], pa.alpha[j])
    sktests[i] <- as.integer(abs(sk(x)) >= cv)
  }
  pwr[j] <- mean(sktests)
}

plot(pa.alpha, pwr, type = "b",
     xlab = bquote(alpha), ylim = c(0,0.1),
     main = "The power estimate of the skewness test of normality against Beta distributions")
abline(h = .05, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(pa.alpha, pwr+se, lty = 3)
lines(pa.alpha, pwr-se, lty = 3)
```

However, the skewness test of normality can be used to tell whether a distribution belongs to t-distribution, especially when the degree of freedom is small. It may relate to its heavy tail feature.

```{r,echo=FALSE}
set.seed(6.72)
c.alpha <- .05
n <- 30
m <- 2000
pa.alpha <- seq(1,30,1)

N <- length(pa.alpha)
pwr <- numeric(N)
#critical value for the skewness test
cv <- qnorm(1-c.alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
for (j in 1:N) {
  sktests <- numeric(m)
  for (i in 1:m) {
    x <- rt(n, pa.alpha[j])
    sktests[i] <- as.integer(abs(sk(x)) >= cv)
  }
  pwr[j] <- mean(sktests)
}

plot(pa.alpha, pwr, type = "b",
     xlab = bquote(alpha),
     main = "The power estimate of the skewness test of normality against t-distributions")
abline(h = .05, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(pa.alpha, pwr+se, lty = 3)
lines(pa.alpha, pwr-se, lty = 3)
```

***

## Question 6.8
Refer to Example 6.16. Repeat the simulation, but also compute the F test of equal variance, at significance level $\hat\alpha\dot=0.055$. Compare the power of the Count Five test and F test for small, medium, and large sample sizes. (Recall that the F test is not applicable for non-normal distributions.)

## Answer
```{r}
set.seed(6.8)
m <- 10000
n <- c(20,50,100)
power.c5 <- power.f <- numeric(3)
sigma1 <- 1
sigma2 <- 1.5
count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
}

# generate samples under H1 to estimate power
for (i in 1:3) {
  power.c5[i] <- mean(replicate(m, expr={
  x <- rnorm(n[i], 0, sigma1)
  y <- rnorm(n[i], 0, sigma2)
  count5test(x, y)
}))
  power.f[i] <- mean(replicate(m, expr={
  x <- rnorm(n[i], 0, sigma1)
  y <- rnorm(n[i], 0, sigma2)
  return(as.integer(var.test(x,y,conf.level = 0.945)$p.value<0.055))
}))
}
power <- cbind(n,power.c5,power.f)
print(power)
```

We choose $n=20,50,100$ as small, medium, and large sample sizes respectively. It seems that F test performs better than Count five test when the samples are generated from normal distribution.

***

## Question 6.C
Repeat Examples 6.8 and 6.10 for Mardia’s multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If X and Y are iid, the multivariate population skewness $\beta_{1,d}$ is defined by Mardia as
$$\beta_{1,d}=E[(X-\mu)^T\Sigma^{-1}(Y-\mu)]^3$$
Under normality, $\beta_{1,d}=0$. The multivariate skewness statistic is
$$b_{1,d}=\frac{1}{n^2}\sum_{i,j=1}^n[(X_i-\bar X)^T\hat\Sigma^{-1}(X_j-\bar X)]^3$$
where $\hat\Sigma$ is the maximum likelihood estimator of covariance. Large values of $b_{1,d}$ are significant. The asymptotic distribution of $nb_{1,d}/6$ is chisquared with $d(d + 1)(d + 2)/6$ degrees of freedom.

## Answer
Repeat Examples 6.8 
```{r}
set.seed(6.81)
d <- 1
n <- c(10, 20, 30, 50, 100, 500) #sample sizes
cv <- qchisq(.95,d*(d+1)*(d+2)/6) #crit. values for each n
sk.mt <- function(x) {
  #computes the sample skewness coeff.
  n <- length(x)
  xbar <- mean(x)
  sigma <- var(x)
  xcenter <- x - xbar
  xsum <- sum((outer(xcenter,xcenter,"*")/sigma)^3)
  return( xsum/n^2 )
}#1 dimension situation
#n is a vector of sample sizes
#we are doing length(n) different simulations
p.reject <- numeric(length(n)) #to store sim. results
m <- 10000 #num. repl. each sim.
for (i in 1:length(n)) {
sktests <- numeric(m) #test decisions
for (j in 1:m) {
x <- rnorm(n[i])
#test decision is 1 (reject) or 0
sktests[j] <- as.integer(sk.mt(x)*n[i]/6 >= cv )
}
p.reject[i] <- mean(sktests) #proportion rejected
}
p.reject
```

Compared to general skewness test of normality, Mardia’s multivariate skewness test also has the problem that when the sample size is small, it tends to underestimates the Type I error.

Repeat Examples 6.10
```{r}
set.seed(6.101)
alpha <- .1
n <- 30
m <- 2500
epsilon <- c(seq(0, .15, .01), seq(.15, 1, .05))
N <- length(epsilon)
pwr <- numeric(N)
#critical value for the skewness test
cv <- qchisq(.9,d*(d+1)*(d+2)/6)
for (j in 1:N) { #for each epsilon
e <- epsilon[j]
sktests <- numeric(m)
for (i in 1:m) { #for each replicate
sigma <- sample(c(1, 10), replace = TRUE,
size = n, prob = c(1-e, e))
x <- rnorm(n, 0, sigma)
sktests[i] <- as.integer(sk.mt(x)*n/6 >= cv )
}
pwr[j] <- mean(sktests)
}
#plot power vs epsilon
plot(epsilon, pwr, type = "b",
xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
```

The whole curve is really like the one in Example 6.10, though the power curve does no longer cross the horizontal line corresponding to $\alpha=0.10$ at just both endpoints but 4 points in the plot.

## Answer 2
We first repeat Example 6.8 which evaluate t1e rate of Mardia’s multivariate skewness test. In our simulation we generate variables following $N(\mu,\Sigma)$, where:
\[\mu=(0,0,0)^{T} , \Sigma= \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \end{bmatrix} \]
```{r}
library(MASS)
Mardia<-function(mydata){
  n=nrow(mydata)
  c=ncol(mydata)
  central<-mydata
  central <- apply(central,2,function(x) x-mean(x))
  sigmah<-t(central)%*%central/n
  a<-central%*%solve(sigmah)%*%t(central)
  b<-sum(a^{3})/(n*n)
  test<-n*b/6
  chi<-qchisq(0.95,c*(c+1)*(c+2)/6)
  as.integer(test>chi)
}

set.seed(1234)
mu <- c(0,0,0)
sigma <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
m=1000
n<-c(10, 20, 30, 50, 100, 500)
#m: number of replicates; n: sample size
a=numeric(length(n))
for(i in 1:length(n)){
  a[i]=mean(replicate(m, expr={
    mydata <- mvrnorm(n[i],mu,sigma) 
    Mardia(mydata)
  }))
}
```

We calculate the t1e when the sample size is 10, 20, 30, 50, 100, 500: 
```{r}
print(a)
```
From the result we can see that t1e rate is close to 0.05 after the sample size is large than 50.


We further repeat Example 6.8 which evaluate the power of Mardia’s multivariate skewness test under distribution $(1-\epsilon)N(\mu_{1},\Sigma_{1})+\epsilon N(\mu_{2},\Sigma_{2})$, where:
\[\mu_{1}=\mu_{2}=(0,0,0)^{T}, \Sigma_{1}=\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \end{bmatrix} 
\Sigma_{2}= \begin{bmatrix}
100 & 0 & 0 \\
0 & 100 & 0 \\
0 & 0 & 100 \end{bmatrix}\]
```{r}
library(MASS)
set.seed(7912)
set.seed(7912)
mu1 <- mu2 <- c(0,0,0)
sigma1 <- matrix(c(1,0,0,0,1,0,0,0,1),nrow=3,ncol=3)
sigma2 <- matrix(c(100,0,0,0,100,0,0,0,100),nrow=3,ncol=3)
sigma=list(sigma1,sigma2)
m=1000
n=50
#m: number of replicates; n: sample size
epsilon <- c(seq(0, .06, .01), seq(.1, 1, .05))
N <- length(epsilon)
pwr <- numeric(N)
for (j in 1:N) { #for each epsilon
  e <- epsilon[j]
  sktests <- numeric(m)
  for (i in 1:m) { #for each replicate
    index=sample(c(1, 2), replace = TRUE, size = n, prob = c(1-e, e))
    mydata<-matrix(0,nrow=n,ncol=3)
    for(t in 1:n){
      if(index[t]==1) mydata[t,]=mvrnorm(1,mu1,sigma1) 
      else mydata[t,]=mvrnorm(1,mu2,sigma2)
    }
    sktests[i] <- Mardia(mydata)
  }
  pwr[j] <- mean(sktests)
}
plot(epsilon, pwr, type = "b",
     xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .05, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
```

When $\epsilon=0$ or $\epsilon=1$ the distribution is multinormal, when $0\leq \epsilon \leq 1$ the
empirical power of the test is greater than 0.05 and highest(close to 1) when $0.1\leq \epsilon \leq 0.3$.

***

## Discussion

* If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?
   
  + What is the corresponding hypothesis test problem?
  + What test should we use? Z-test, two-sample t-test, paired-t
test or McNemar test?
  + What information is needed to test your hypothesis?

## Answer
  1. $H_0:$the powers for these two methods are not different.
     $H_1:$the powers for these two methods are different.
     
  2. We can use either Z-test, paired-t test or McNemar test. Note that the power computed from the two methods are highly closed and related, we could not use the two-sample t-test which requires the independence of two samples.
  
  3. First we need every single result about whether accpted(0) or rejected(1) for two methods of the 10,000 experiments. 
  For Z-test and paired-t test, we need to subtract the results number of each experiments, and then compute the mean and variance of the sample after subtraction. And then use the statistics below.
  
$$\left| \frac{\bar D-(\mu_1-\mu_2)}{S/\sqrt n} \right |\geqslant t_{\alpha/2}(n-1)$$
or
$$\left| \frac{\bar D-(\mu_1-\mu_2)}{S/\sqrt n} \right |\geqslant z_{\alpha/2}$$
Where in this case we have $\bar D=0$, $S$ represents the standard deviation of subtracted results of paired observations in the two samples.$\mu_1=0.651, \mu_2=0.676$. Generally we take $\alpha=0.05$.

For McNemar test, we need to assign numbers into 2*2 contingency table according to the results whether both methods are accepted, both are rejected or one is accepted while another is not. And then use the statistics below.
$$\frac{(b-c)^2}{b+c} \sim \chi^2(1)$$
Where b represents the total number of experiments that method A accepted while method B not, and c vice versa.

#Homework 6
## Question 7.1
Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

## Answer
The jackknife estimate of the bias of the correlation statistic is -0.00647. And the standard error of the correlation statistic is 0.1425.

```{r,warning=FALSE}
library(bootstrap) #for the law data
data("law")
n <- nrow(law)
theta.hat <- cor(law$LSAT, law$GPA)
##Estimate the bias
theta.jack <- numeric(n)
for (i in 1:n)
theta.jack[i] <- cor(law$LSAT[-i], law$GPA[-i])
bias <- (n - 1) * (mean(theta.jack) - theta.hat)
##Estimate the standard error
se <- sqrt((n-1) * mean((theta.jack - mean(theta.jack))^2))
print(list("bias"=bias,"se"=se))
```

***

## Question 7.5
Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures $1/\lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

## Answer
The induction of the MLE of the hazard rate λ presents below:
\[\begin{align}
\mathcal L(\lambda|x_i)&=nlog\lambda-\lambda\sum_{i=1}^nx_i\\
\mathcal L^{'}(\lambda|x_i)&=\frac{n}{\lambda}-\sum_{i=1}^nx_i=0\\
\therefore\hat\lambda&=\frac{1}{\bar X}
\end{align}
\]
Thus, the mean time between failures $1/\lambda$ can be computed by $\bar X$
```{r,warning=FALSE}
library(boot)
data("aircondit")
hour <- aircondit$hours
lambda.boot <- function(dat,ind){
  y <- dat[ind]
  mean(y)
}
boot.obj <- boot(data = hour,statistic = lambda.boot, R=2000)
print(boot.obj)
print(boot.ci(boot.obj,conf = 0.95) )
hist(boot.obj$t)

```

All four intervals cover the the mean $\frac{1}{\lambda}=108.08$ of time between failures. One reason for the difference in the percentile and normal confidence intervals could be that the sampling distribution of mean statistic is not close to normal (see the histogram). When the sampling distribution of the statistic is approximately normal, the percentile interval will agree with the normal interval.

It seems that BCa confidence intervals are more deviated from normal confidence intervals, which induces constant variance $\sigma^2(\hat\theta)$ possibly depend on the parameter $\theta$. It make sense because population distribution seems like exponential distribution which are explained in Exercise 7.4. As for exponential distribution, its expectation equals to its variance.

***

## Question 7.8
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat\theta$.

## Answer

```{r,warning=FALSE}
data("scor")
n <- nrow(scor)

Contribution<-function(mydata){
  n=nrow(mydata)
  c=ncol(mydata)
  central<-mydata
  central <- apply(central,2,function(x) x-mean(x))
  sigmah<-t(central)%*%central/n
  eigenh <- eigen(sigmah)
  theta.hat <- eigenh$values[1]/sum(eigenh$values)
  theta.hat
}

theta.hat <- Contribution(scor)
theta.jack <- numeric(n)
for (i in 1:n)
theta.jack[i] <- Contribution(scor[-i,])
bias <- (n - 1) * (mean(theta.jack) - theta.hat)
##Estimate the standard error
se <- sqrt((n-1) * mean((theta.jack - mean(theta.jack))^2))
print(list("bias"=bias,"se"=se))
```
The jackknife estimate of the bias of $\hat\theta$ is 0.00106. And the standard error of $\hat\theta$ is 0.0496.
```{r,echo=FALSE,warning=FALSE}
B <- 2000 #larger for estimating bias
theta.b <- numeric(B)
tt <- scor
for (b in 1:B) {
i <- sample(1:n, size = n, replace = TRUE)
LSAT <- tt[i,]
theta.b[b] <- Contribution(LSAT)
}
bias <- mean(theta.b - theta.hat)
bias
```
Compared to bootstrap method, the bias of $\hat\theta$ is similar.

***

## 7.11
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

## Answer

```{r,warning=FALSE,message=FALSE}
library(DAAG); attach(ironslag)
a <- seq(10, 40, .1)

n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n)
# for n-fold cross validation
# fit models on leave-one-out samples
for (k in 1:(n-1)) {
  for (j in (k+1):n) {
    y <- magnetic[-c(k,j)]
    x <- chemical[-c(k,j)]
    
    J1 <- lm(y ~ x)
    yhat1 <- J1$coef[1] + J1$coef[2] * chemical[c(k,j)]
    e1[k] <- e1[k] + sum((magnetic[c(k,j)] - yhat1)^2)/2
    
    J2 <- lm(y ~ x + I(x^2))
    yhat2 <- J2$coef[1] + J2$coef[2] * chemical[c(k,j)] +
      J2$coef[3] * chemical[c(k,j)]^2
    e2[k] <- e2[k] + sum((magnetic[c(k,j)] - yhat2)^2)/2
    
    J3 <- lm(log(y) ~ x)
    logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[c(k,j)]
    yhat3 <- exp(logyhat3)
    e3[k] <- e3[k] + sum((magnetic[c(k,j)] - yhat3)^2)/2
    
    J4 <- lm(log(y) ~ log(x))
    logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[c(k,j)])
    yhat4 <- exp(logyhat4)
    e4[k] <- e4[k] + sum((magnetic[c(k,j)] - yhat4)^2)/2
  }
}
t <- n*(n-1)/2
c(sum(e1)/t,sum(e2)/t,sum(e3)/t,sum(e4)/t)
```
According to the prediction error criterion, Model 2, the quadratic model, would be the best fit for the data.

## Answer 2
 
In Example 7.18, leave-one-out ($n$-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

## Answer
```{r}
library(DAAG)
attach(ironslag)
n <- length(magnetic)
e1 <- numeric(n*(n-1)/2)
e2 <- numeric(n*(n-1)/2)
e3 <- numeric(n*(n-1)/2)
e4 <- numeric(n*(n-1)/2)
count <- 0
for (i in 1:(n-1))
  for (j in (i+1):n) {
    count <- count+1
    y <- magnetic[-c(i,j)]
    x <- chemical[-c(i,j)]
    
    P1 <- lm(y~x)
    y1_1 <- chemical[i]*P1$coef[2] + P1$coef[1]
    y1_2 <- chemical[j]*P1$coef[2] + P1$coef[1]
    e1[count] <- (magnetic[i]-y1_1)^2+(magnetic[j]-y1_2)^2
    
    P2 <- lm(y~x+I(x^2))
    y2_1 <- P2$coef[1] + P2$coef[2] * chemical[i] + P2$coef[3] * chemical[i]^2
    y2_2 <- P2$coef[1] + P2$coef[2] * chemical[j] + P2$coef[3] * chemical[j]^2
    e2[count] <- (magnetic[i]-y2_1)^2+(magnetic[j]-y2_2)^2
    
    P3 <- lm(log(y)~x)
    y3_1 <- exp(P3$coef[1] + P3$coef[2] * chemical[i])
    y3_2 <- exp(P3$coef[1] + P3$coef[2] * chemical[j])
    e3[count] <- (magnetic[i]-y3_1)^2+(magnetic[j]-y3_2)^2
    
    P4 <- lm(log(y)~log(x))
    y4_1 <- exp(P4$coef[1] + P4$coef[2] * log(chemical[i]))
    y4_2 <- exp(P4$coef[1] + P4$coef[2] * log(chemical[j]))
    e4[count] <- (magnetic[i]-y4_1)^2+(magnetic[j]-y4_2)^2
  }

e = c(mean(e1)/2,mean(e2)/2,mean(e3)/2,mean(e4)/2)
matrix(e, nrow=1,
       dimnames=list("prediction error", c("Linear","Quadratic"," Exponential","Log-Log")))
detach(ironslag)
```

#Homework 7
## Question 8.3
The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

## Answer
We can use the permutation method to select a subset whose size is equal to the small one, from the larger size sample. And then use the Count5 Test.
```{r}
library(boot)
set.seed(123)
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer(max(c(outx, outy)) > 5))
}

permC <- function(n1,n2,mu1=0,mu2=0,sigma1=1,sigma2=1,R = 100){
  if(n1>n2)stop("n1 should be less than n2")
  reps <- numeric(R)
  x <- rnorm(n1,mu1,sigma1)
  y <- rnorm(n2,mu2,sigma2)
  for (i in 1:R) {
    #generate indices k for the first sample
    k <- sample(1:n2, size = n1, replace = FALSE)
    x1 <- x
    y1 <- y[k] #complement of x1
    reps[i] <- count5test(x1, y1)
  }
  mean(reps)
}

n1 <- 20
n2 <- 40
m <- 1000
alphahat <- mean(replicate(m, expr=permC(n1,n2)))
print(alphahat)

```

It's really clsoe to the significance level 0.05.

***

## Discussion(Equal disttribution test)
* Design experiments for evaluating the performance of the NN, energy, and ball methods in various situations.
  + Unequal variances and equal expectations
  + Unequal variances and unequal expectations
  + Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)
  + Unbalanced samples (say, 1 case versus 10 controls)
  + Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8).

## Answer
1. We can generate random X and Y from these two distribution below like:
$X\sim N(\mu_1,\Sigma_1)$, $Y\sim N(\mu_2,\Sigma_2)$
assuming they have Unequal variances and equal expectations

```{r,warning=FALSE,message=FALSE}
library(MASS)
library(RANN)
library(energy)
library(Ball)

alpha <- 0.05
sim <- matrix(0,41,4)

for (i in 0:40) {
  epsilon <- (i+10)*.1
  mu1 <- c(0,0)
sigma1 <- epsilon*diag(nrow = 2)
mu2 <- c(0,0)
sigma2 <- diag(nrow = 2)
X <- mvrnorm(40,mu1,sigma1)
Y <- mvrnorm(40,mu2,sigma2)

#NN test
Z <- rbind(X, Y)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) # what's the first column?
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}
set.seed(12345); N <- c(nrow(X), nrow(Y))
boot.obj <- boot(data = Z, statistic = Tn, R = 999,
                 sim = "permutation", sizes = N,k=3)
ts <- c(boot.obj$t0,boot.obj$t)
p.value1 <- mean(ts>=ts[1])

#energy test
boot.obs <- eqdist.etest(Z, sizes=N, R=999)
p.value2 <- boot.obs$p.value

#ball test
p.value3 = bd.test(x = X, y = Y, num.permutations = 999)

p <- c(p.value1,p.value2,p.value3$p.value)
names(p) <- c('NN test','energy test','ball test')
sim[i+1,] <- c(epsilon,p)
}
 
plot(sim[,1], sim[,2], ylim = c(0, 1), type = "l",
xlab = bquote(epsilon), ylab = "power")
lines(sim[,1], sim[,3], lty = 2)
lines(sim[,1], sim[,4], lty = 4)
abline(h = alpha, lty = 3)
legend("topright", 1, c('NN test','energy test','ball test'),
lty = c(1,2,4), inset = .02)

```

The p-value of all test are significant, and it seems ball test performs better.

2. We can generate random X and Y from these two distribution below like:
$X\sim N(\mu_1,\Sigma_1)$, $Y\sim N(\mu_2,\Sigma_2)$
assuming they have Unequal variances and unequal expectations

```{r}

sim <- matrix(0,41,4)

for (i in 0:40) {
  epsilon <- (i+40)*.025
  mu1 <- c(epsilon-1,2*(epsilon-1))
sigma1 <- epsilon*diag(nrow = 2)
mu2 <- c(0,0)
sigma2 <- diag(nrow = 2)
X <- mvrnorm(40,mu1,sigma1)
Y <- mvrnorm(40,mu2,sigma2)

#NN test
Z <- rbind(X, Y)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) # what's the first column?
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}
set.seed(12345); N <- c(nrow(X), nrow(Y))
boot.obj <- boot(data = Z, statistic = Tn, R = 999,
                 sim = "permutation", sizes = N,k=3)
ts <- c(boot.obj$t0,boot.obj$t)
p.value1 <- mean(ts>=ts[1])

#energy test
boot.obs <- eqdist.etest(Z, sizes=N, R=999)
p.value2 <- boot.obs$p.value

#ball test
p.value3 = bd.test(x = X, y = Y, num.permutations = 999)

p <- c(p.value1,p.value2,p.value3$p.value)
names(p) <- c('NN test','energy test','ball test')
sim[i+1,] <- c(epsilon,p)
}
 
plot(sim[,1], sim[,2], ylim = c(0, 1), type = "l",
xlab = bquote(epsilon), ylab = "p-value")
lines(sim[,1], sim[,3], lty = 2)
lines(sim[,1], sim[,4], lty = 4)
abline(h = alpha, lty = 3)
legend("topright", 1, c('NN test','energy test','ball test'),
lty = c(1,2,4), inset = .02)
```

The energy test performs better if the expectation changes along with variance.

3. We can generate random X and Y from these two distribution below like:
$X\sim t(1)$, $F_Y(y) = \alpha F_{X_1}(x)+(1-\alpha)F_{X_2}(x)$
assuming $X_1,X_2$ have diffirent distributions.

```{r}

sim <- matrix(0,41,4)

for (i in 0:40) {
  epsilon <- i*.025
X <- rt(40,1)
x1 <- rnorm(40,0,1)
x2 <- rnorm(40,4,2)
u <- runif(40)
k <- as.integer(u>epsilon)
Y <- k*x1+(1-k)*x2

#NN test
Z <- c(X, Y)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) # what's the first column?
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}
set.seed(12345); N <- c(length(X), length(Y))
boot.obj <- boot(data = Z, statistic = Tn, R = 999,
                 sim = "permutation", sizes = N,k=3)
ts <- c(boot.obj$t0,boot.obj$t)
p.value1 <- mean(ts>=ts[1])

#energy test
boot.obs <- eqdist.etest(Z, sizes=N, R=999)
p.value2 <- boot.obs$p.value

#ball test
p.value3 = bd.test(x = X, y = Y, num.permutations = 999) 

p <- c(p.value1,p.value2,p.value3$p.value)
names(p) <- c('NN test','energy test','ball test')
sim[i+1,] <- c(epsilon,p)
}
 
plot(sim[,1], sim[,2], ylim = c(0, 1), type = "l",
xlab = bquote(epsilon), ylab = "p-value")
lines(sim[,1], sim[,3], lty = 2)
lines(sim[,1], sim[,4], lty = 4)
abline(h = alpha, lty = 3)
legend("topright", 1, c('NN test','energy test','ball test'),
lty = c(1,2,4), inset = .02)
```

It seems energy test performs the best and ball test is also good enough. However the NN test can detect the difference between the t-distribution and bimodel distribution only when the bi-leak patterns is really clear.

4. We can generate random X and Y from these two distribution below like:
$X\sim N(\mu_1,\Sigma_1)$, $Y\sim N(\mu_2,\Sigma_2)$
assuming they have Unbalanced samples.

```{r}

sim <- matrix(0,21,4)

for (i in 0:20) {
  epsilon <- i+2
  mu1 <- c(0,0)
sigma1 <- 4*diag(nrow = 2)
mu2 <- c(0,0)
sigma2 <- diag(nrow = 2)
X <- mvrnorm(epsilon,mu1,sigma1)
Y <- mvrnorm(epsilon*10,mu2,sigma2)

#NN test
Z <- rbind(X, Y)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) # what's the first column?
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}
set.seed(12345); N <- c(nrow(X), nrow(Y))
boot.obj <- boot(data = Z, statistic = Tn, R = 999,
                 sim = "permutation", sizes = N,k=3)
ts <- c(boot.obj$t0,boot.obj$t)
p.value1 <- mean(ts>=ts[1])

#energy test
boot.obs <- eqdist.etest(Z, sizes=N, R=999)
p.value2 <- boot.obs$p.value

#ball test
p.value3 = bd.test(x = X, y = Y, num.permutations = 999)

p <- c(p.value1,p.value2,p.value3$p.value)
names(p) <- c('NN test','energy test','ball test')
sim[i+1,] <- c(epsilon,p)
}
 
plot(sim[,1], sim[,2], ylim = c(0, 1), type = "l",
xlab = bquote(epsilon), ylab = "p-value")
lines(sim[,1], sim[,3], lty = 2)
lines(sim[,1], sim[,4], lty = 4)
abline(h = alpha, lty = 3)
legend("topright", 1, c('NN test','energy test','ball test'),
lty = c(1,2,4), inset = .02)
```

It seems in the unbalanced sample size situation, with the sample size increasing (pro ratas), the energy test is the most robust and the ball test the second robust.

#Homework 8
## Question 9.4
Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain

## Answer
The density function of Laplace distribution is:
$$f(x)=\frac{1}{2}e^{-|x|} \quad,x\in\mathbb R$$
and the distributon function is:
$$F(x)=\begin{cases}
\frac{1}{2}e^x,\quad x<0\\
1-\frac{1}{2}e^{-x},\quad x\geqslant 0
\end{cases}$$

So,the 97.5% and 2.5% quantile of Laplace distribution are close to +3 and -3 respectively, according to which we can give the reference line.

```{r}
library(knitr)
set.seed(12345)
dLaplace <- function(x){
  y <- 0.5*exp(-abs(x))
  return(y)
}

rw.Metropolis.L <- function(sigma, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (dLaplace(y)/dLaplace(x[i-1])))
      x[i] <- y  
    else {
      x[i] <- x[i-1]
      k <- k + 1
    }
  }
  return(list(x=x, k=k))
}

n <- 4  #degrees of freedom for target Student t dist.
N <- 2000
sigma <- c(.05, .5, 2,  16)

x0 <- 25
rw1 <- rw.Metropolis.L(sigma[1], x0, N)
rw2 <- rw.Metropolis.L(sigma[2], x0, N)
rw3 <- rw.Metropolis.L(sigma[3], x0, N)
rw4 <- rw.Metropolis.L(sigma[4], x0, N)
reject <- c(rw1$k, rw2$k, rw3$k, rw4$k)
no.reject <- NULL
#number of candidate points rejected
no.reject <- data.frame(sigma=sigma,no.reject=reject,accepted.rate=(N-reject)/N)
knitr::kable(no.reject)

#par(mfrow=c(2,2))  #display 4 graphs together
alpha <- 0.025
refline <- c(log(2*alpha),-log(2)-log(alpha))
rw <- cbind(rw1$x, rw2$x, rw3$x,  rw4$x)
for (j in 1:4) {
  plot(rw[,j], type="l",
       xlab=bquote(sigma == .(round(sigma[j],3))),
       ylab="X", ylim=range(rw[,j]))
  abline(h=refline)
}
#par(mfrow=c(1,1)) #reset to default
```
***

## Question 9.4(supplemented)
For Exercise 9.4, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat R$ < 1.2.

## Answer
We take $\sigma=2$ for example.The value of $\hat R$ is below 1.2 within 1000 iterations and below 1.1 within 2000 iterations

```{r}
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  
  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
}

sigma <- 2
k <- 4          #number of chains to generate
n <- 15000      #length of chains

#choose overdispersed initial values
x0 <- c(-10, -5, 5, 10)

#generate the chains
set.seed(12345)
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k) {
  dd <- rw.Metropolis.L(sigma=sigma, N=n, x0=x0[i])
  dd3 <- dd$x
  X[i, ] <- dd3
}

#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi)) psi[i,] <- psi[i,] / (1:ncol(psi))

#plot the sequence of R-hat statistics
rhat <- rep(0, n)
for (j in 2:n)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[2:n], type="l", xlab="", ylab="R",ylim = c(1,1.3),xlim = c(0,8000))
abline(h=1.2, lty=2)

```
***

## Question 11.4
Find the intersection points $A(k)$ in $(0,\sqrt k )$
$$S_{k-1}(a)=P\left( t(k-1)>\sqrt\frac{a^2(k-1)}{k-a^2} \right)$$
and
$$S_{k}(a)=P\left( t(k)>\sqrt\frac{a^2k}{k+1-a^2} \right)$$
for $k = 4 : 25, 100, 500, 1000$, where $t(k)$ is a Student t random variable with k degrees of freedom. (These intersection points determine the critical values for a t-test for scale-mixture errors proposed by Sz´ekely [260].)

## Answer
```{r}
k1 <- c(4:25,100,500,1000)
f <- function(a,k=4){
  x <- sqrt((a^2*(k-1))/(k-a^2))
  y <- sqrt((a^2*k)/(k+1-a^2))
  Sk1 <- 1-pt(x,df=k-1)
  Sk2 <- 1-pt(y,df=k)
  Sk1-Sk2
}   
res <- uniroot(f,c(1,2))
res
aa <- seq(-2,2,0.01)
plot(aa,f(aa,k=4),type = "l")
abline(h=0,lty=2)
points(1.49,0,cex=2,col="red")
```

From the plot,we find that the solution is approximately in the interval (1,2).


```{r}
res <- numeric(25)
for (i in 1:length(k1)) {
  f <- function(a,k=k1[i]){
    x <- sqrt((a^2*(k-1))/(k-a^2))
    y <- sqrt((a^2*k)/(k+1-a^2))
    Sk1 <- 1-pt(x,df=k-1)
    Sk2 <- 1-pt(y,df=k)
    Sk1-Sk2
  }  
  sol <- round(uniroot(f,c(1,2))$root,5)
  res[i] <- sol
}
sol <- cbind(k1,res)
print(sol)
```

#Homework 9
## A-B-O blood type problem

```{r,echo=FALSE,message=FALSE}
Genotype <- c("AA","BB","OO","AO","BO","AB","sum")
Frequency <- c("p^2","q^2","r^2","2pq","2pr","2qr","1")
Count <- c("nAA","nBB","nOO","nAO","nBO","nAB","n")
sheet <- cbind(Genotype,Frequency,Count)
library(knitr)
kable(t(sheet))
```
* Observed data: nA· = nAA + nAO = 444 (A-type),
nB· = nBB + nBO = 132 (B-type), nOO = 361 (O-type),
nAB = 63 (AB-type).
+ Use EM algorithm to solve MLE of p and q (consider missing
data nAA and nBB).
+ Record the values of p and q that maximize the conditional
likelihood in each EM steps, calculate the corresponding
log-maximum likelihood values (for observed data), are they
increasing?

## Answer
The log likelihood function:
$$Q=log\mathcal L_c(p,q|x)=n_{AA}lnp^2+n_{BB}lnq^2+n_{OO}lnr^2+n_{AO}ln2pr+n_{BO}ln2qr+n_{AB}lnpq\\
r=1-p-q
$$
The distribution of $n_{AA},n_{BB}$ given $n_{A\cdot},n_{B\cdot}$

\[\begin{align}
n_{AA}|n_{A\cdot}\sim B(n_{A\cdot},\frac{p^2}{p^2+2pr})\\
n_{BB}|n_{B\cdot}\sim B(n_{B\cdot},\frac{q^2}{q^2+2qr})
\end{align}
\]
The partial derivative of log likelihood function Q:
\[\begin{align}
\frac{\partial Q}{\partial p}=\frac{2n_{AA}+n_{AO}+n_{AB}}{p}-\frac{2n_{OO}+n_{AO}+n_{BO}}{r}\\
\frac{\partial Q}{\partial q}=\frac{2n_{BB}+n_{BO}+n_{AB}}{q}-\frac{2n_{OO}+n_{AO}+n_{BO}}{r}
\end{align}
\]

Solve the equation with$r=1-p-q$
\[\begin{align}
r=\frac{2n_{OO}+n_{BO}+n_{AO}}{2n}\\
p=\frac{2n_{AA}+n_{AO}+n_{AB}}{2n}\\
q=\frac{2n_{BB}+n_{BO}+n_{AB}}{2n}
\end{align}
\]

```{r}
bloodtype<-function(p,n.obs){
  n<-sum(n.obs)
  nA<-n.obs[1]
  nB<-n.obs[2]
  nAB<-n.obs[3]
  nOO<-n.obs[4]
  cat(p,"\n")
  pAt<-pBt<-pOt<-Q <- rep(0,20)
  pAt[1]<-p[1]
  pBt[1]<-p[2]
  pOt[1]<-1-p[1]-p[2]
  Q[1] <- 0
  for(i in 2:20){
  pA.old<-pAt[i-1]
  pB.old<-pBt[i-1]
  pO.old<-pOt[i-1]
  
  nAA <- nA * pA.old^2/(pA.old^2+2*pA.old*pO.old)
  nAO <- nA * 2*pA.old*pO.old/(pA.old^2+2*pA.old*pO.old)
  nBB <- nB * pB.old^2/(pB.old^2+2*pB.old*pO.old)
  nBO <- nB * 2*pB.old*pO.old/(pB.old^2+2*pB.old*pO.old)
  
  pAt[i]<-(2*nAA+nAO+nAB)/(2*n)
  pBt[i]<-(2*nBB+nBO+nAB)/(2*n)
  pOt[i]<-(2*nOO+nAO+nBO)/(2*n)
  Q[i] <- 2*nAA*log(pAt[i])+2*nBB*log(pBt[i])+2*nOO*log(pOt[i])+
    nAO*log(2*pAt[i]*pOt[i])+nBO*log(2*pBt[i]*pOt[i])+nAB*log(2*pAt[i]*pBt[i])
  }
  return(list(pAt=pAt,pBt=pBt,pOt=pOt,Q=Q))
}
n.obs<-c(444,132,63,361) # observed data,n_A,n_B,n_AB,n_OO
p<-c(1/3,1/3)
a<-bloodtype(p,n.obs)
print(a)
```
As the result Q shows,the corresponding log-maximum likelihood values are increasing.

***

## Exercises 3 (page 204, Advanced R).
Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:
```{r,eval=FALSE}
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
```
## Answer
```{r}
attach(mtcars)
formulas = list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)

for (i in 1:4) {print(summary(lm(formulas[[i]])))}
lapply(formulas, function(x) return (summary(lm(x))))
```


***

## Excecises 3 and 6 (page 213-214, Advanced R).
### Note: the anonymous function is defined in Section 10.2 (page 181, Advanced R)

3. The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial. 

```{r,eval=FALSE}
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
```

Extra challenge: get rid of the anonymous function by using [[ directly.

```{r}
set.seed(12345)
sapply(1:100, function(x) return (t.test(rpois(10, 10), rpois(7, 10))$p.value))
```

6. Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

## Answer
```{r}
Mapvapplay<-function (f,n,type, ...) {  
 #n:the length of output  
 f <- match.fun(f) 
 fM=Map(f, ...) 
 if(type=="numeric") return(vapply(fM,cbind,numeric(n))) 
 else if (type=="character") return(vapply(fM,cbind,character(n))) 
 else if (type=="logical") return(vapply(fM,cbind,logical(n))) 
 } 
```

#Homework 10
## Question 
1. Write an Rcpp function for Exercise 9.4 (page 277, Statistical
Computing with R).

Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

2. Compare the corresponding generated random numbers with
those by the R function you wrote before using the function
“qqplot”.

3. Campare the computation time of the two functions with the
function “microbenchmark”.

4. Comments your results.

## Answer


```{r}
library(Rcpp)
sourceCpp("../src/rw_Metropolis.cpp")

dLaplace <- function(x){
  y <- 0.5*exp(-abs(x))
  return(y)
}
rw.Metropolis.L <- function(sigma, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (dLaplace(y)/dLaplace(x[i-1])))
      x[i] <- y  
    else {
      x[i] <- x[i-1]
      k <- k + 1
    }
  }
  return(list(x=x, k=k))
}

n <- 4  #degrees of freedom for target Student t dist.
N <- 2000
x0 <- 25
sigma <- c(.05, .5, 2,  16)

rw2L <- rw.Metropolis.L(sigma[3], x0, N)
rw2C <- rw_Metropolis(sigma[3], x0, N)
qqplot(rw2C$x[-(1:800)],rw2L$x[-(1:800)])

library(microbenchmark)
ts <- microbenchmark(rw2L = rw.Metropolis.L(sigma[2], x0, N), rw2C = rw_Metropolis(sigma[2], x0, N))
summary(ts)[,c(1,3,5,6)]


```

Summary:the corresponding generated random numbers are close to those by the R function, but the generating speed is more fast.
